{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#script to web scrape the hansard website.\n",
    "#1-filter by document type speeches\n",
    "#2-get the list of all MPs from the 47 parliament onwards\n",
    "#3-download all speeches from each MP, once the MP name is in the title of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 #parse html from web pages\n",
    "from lxml import html\n",
    "import requests #load web pages\n",
    "from urllib2 import urlopen #load web pages\n",
    "import re\n",
    "import mechanize\n",
    "import time\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using request as loader and BeautifulSoup as parser\n",
    "#http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python\n",
    "#4721 pages remaining (total of 94429 documents)\n",
    "\n",
    "root_url = 'https://www.parliament.nz'\n",
    "index_url = root_url + '/en/pb/hansard-debates/rhr/search'\n",
    "search_url = index_url + '?Criteria.PageNumber='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to write list of url to txt file\n",
    "def list_txt_file(filename,z):\n",
    "    f = open('/home/dvasques/Dropbox (Complex systems)/TPM Research Bootcamp/speech-data/' + filename + '.txt', 'wb')\n",
    "    for url in z:\n",
    "        url = url.encode('utf-8').strip()\n",
    "        f.write(\"%s\\n\" % url)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to read url from file and create a list\n",
    "def url_from_csv(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        speech_urls = list(reader)\n",
    "    \n",
    "    return speech_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get all the MPs from Hansard's search website\n",
    "def get_mps_list(index_url):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    response = requests.get(index_url)\n",
    "    soup = bs4.BeautifulSoup(response.content,'lxml')\n",
    "    mps_list = []\n",
    "    for option in soup.select('select#criteria_MemberOfParliament option[value]'):\n",
    "        mps_list.append(option.attrs.get('value'))\n",
    "        \n",
    "    print 'Running time to get MPs list: ' + str(time.time() - t0)\n",
    "        \n",
    "    return mps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to create folders with MPs' names\n",
    "def create_mp_folder(mps_list):\n",
    "    root_path = '/home/dvasques/Desktop/speech-data-d/'\n",
    "    folder = mps_list\n",
    "    os.mkdir(os.path.join(root_path,folder))\n",
    "    #for folder in folders:\n",
    "        #os.mkdir(os.path.join(root_path,folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to write the extracted text to a txt tile\n",
    "def write_txt_file(filename,z):\n",
    "    f = open('/home/dvasques/Desktop/speech-data-d/' + filename + '.txt', 'wb')\n",
    "    f.write(z)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to find positioning of dash (to get only the mp name)\n",
    "def dash_positions(url):\n",
    "    \n",
    "    s = url\n",
    "    c = '-'\n",
    "    positions = [pos for pos, char in enumerate(s) if char == c]\n",
    "    #print speech_urls[205][0].index('-')\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to write all url content into txt files\n",
    "def speech_to_txt(speech_urls):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in range(len(speech_urls)):\n",
    "    #for i in range(0,100,1):\n",
    "        \n",
    "        response = requests.get(speech_urls[i][0])\n",
    "        soup = bs4.BeautifulSoup((response.content),'lxml')\n",
    "        z = s.find(\"div\", {\"class\": \"section\"}).get_text()\n",
    "        zencode = z.encode('utf-8').strip()\n",
    "        #for strong_tag in z[0].find_all('strong'):\n",
    "            #print strong_tag.text\n",
    "        filename = speech_urls[i][61:86] + '-' + speech_urls[i][87:]\n",
    "        write_txt_file(filename,zencode)\n",
    "        #print strong_tag\n",
    "        \n",
    "        print i\n",
    "        \n",
    "    print 'Running time to get all documents: ' + str(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to pre-process raw data, writing content organized in folders\n",
    "def speech_to_txt2(speech_urls, parl, year, date, key, mps, mps_dict): #with lists\n",
    "    #with lists is easier to create nodes with attributes\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    #for i in range(len(speech_urls)):\n",
    "    for i in range(0,len(speech_urls),1):\n",
    "        \n",
    "        url = speech_urls[i][0]\n",
    "    \n",
    "        #when mp name starts with digit it is not a valid speech from MP\n",
    "        if len(url) > 120:\n",
    "        \n",
    "            if not speech_urls[i][0][120].isdigit():\n",
    "\n",
    "                #get speech if url has text\n",
    "                if i < 494:\n",
    "                    original_url = speech_urls[i][0][:94] + speech_urls[i][0][96:118] + '0' + speech_urls[i][0][118:]\n",
    "                    response = requests.get(original_url)\n",
    "                else:     \n",
    "                    response = requests.get(url)\n",
    "\n",
    "                soup = bs4.BeautifulSoup((response.content),'lxml')\n",
    "                z = soup.find(\"div\", {\"class\": \"section\"})\n",
    "                if z is not None:\n",
    "                    z = soup.find(\"div\", {\"class\": \"section\"}).get_text()\n",
    "                    zencode = z.encode('utf-8').strip()\n",
    "\n",
    "\n",
    "\n",
    "                    parl_s = url[94:96]\n",
    "                    parl.append(parl_s)\n",
    "                    year_s = url[102:106]\n",
    "                    year.append(year_s)\n",
    "                    date_s = url[102:110]\n",
    "                    date.append(date_s)\n",
    "                    key_s = url[111:119]\n",
    "                    key.append(key_s)\n",
    "\n",
    "\n",
    "                    #find positions of each dash in the url, calling from function\n",
    "                    positions = dash_positions(url)\n",
    "                    if len(positions)>3:\n",
    "                        mp_name = url[120:positions[3]] #string\n",
    "                        mps.append(mp_name)\n",
    "                    else:\n",
    "                        mp_name = url[120:]\n",
    "                        mps.append(mp_name)\n",
    "\n",
    "\n",
    "                    #creating a dictionary of urls for each mp\n",
    "                    if mp_name not in mps_dict.keys():\n",
    "                        create_mp_folder(mp_name)\n",
    "                        mps_dict[mp_name] = [url]\n",
    "                    else:\n",
    "                        mps_dict[mp_name].append(url) \n",
    "\n",
    "\n",
    "                    filename = mp_name + '/' + parl_s + '-' + date_s + '-' + key_s\n",
    "                    write_txt_file(filename,zencode)\n",
    "                    #print strong_tag\n",
    "\n",
    "                    print i\n",
    "                \n",
    "            else:\n",
    "                print 'No text: ', speech_urls[i][0]\n",
    "        \n",
    "        else:\n",
    "            print speech_urls[i][0]\n",
    "        \n",
    "    print 'Running time to get all documents: ' + str(time.time() - t0)\n",
    "    \n",
    "    return parl, year, date, key, mps, mps_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get urls from csv file\n",
    "speech_urls = url_from_csv('/home/dvasques/Desktop/speech_urls.csv')\n",
    "print 'Number of urls: ', len(speech_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#correct urls to extract information from url strings\n",
    "for i in range(0,494,1):\n",
    "    speech_urls[i][0] = speech_urls[i][0][:94] + '51' + speech_urls[i][0][94:117] + speech_urls[i][0][118:]\n",
    "\n",
    "print speech_urls[0][0]\n",
    "print speech_urls[0][0][:94] + speech_urls[0][0][96:118] + '0' + speech_urls[0][0][118:]\n",
    "\n",
    "print speech_urls[493][0]\n",
    "print speech_urls[494][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize lists and dictionary\n",
    "parl = []\n",
    "year = []\n",
    "date = []\n",
    "key = []\n",
    "mps = []\n",
    "mps_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#call function\n",
    "parl, year, date, key, mps, mps_dict = speech_to_txt2(speech_urls, parl, year, date, key, mps, mps_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
